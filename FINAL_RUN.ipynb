{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ao6plRr1EaFE",
    "outputId": "449934ad-484d-4f1f-dbac-050bba3736d6"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "FOLDERNAME = \"/content/drive/MyDrive/Fall_2025/Deep_Learning/DL_Project/Final Submission/\"\n",
    "#FOLDERNAME = \"/content/drive/MyDrive/Deep Learning/DL_Project/Final Submission/\"\n",
    "assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n",
    "\n",
    "# Now that we've mounted your Drive, this ensures that\n",
    "# the Python interpreter of the Colab VM can load\n",
    "# python files from within it.\n",
    "import sys\n",
    "sys.path.append('/content/drive/My Drive/{}'.format(FOLDERNAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DceMkDpgOtYE"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import sampler\n",
    "from torchvision import models, transforms\n",
    "\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import io\n",
    "from IPython.display import display\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "\n",
    "# import torchvision.datasets as dset\n",
    "# import torchvision.transforms as T\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "USE_GPU = True\n",
    "dtype = torch.float32 # We will be using float throughout this tutorial.\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XubPMuJrk9Fk",
    "outputId": "20f371ad-bd75-40c8-eb13-6bb19d5ab5b5"
   },
   "outputs": [],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F8WO2AeYk_3Z"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eUHIqPZe4bku"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xi6jOrUQ4UME"
   },
   "outputs": [],
   "source": [
    "from torchvision.models import resnet50, ResNet50_Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ag6EondJk0ja"
   },
   "source": [
    "Load in EMoSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "812bda884dfa4e299a4e88b72f96572f",
      "d3452134fdd1406a8e23145b8d59d8f1",
      "95cde655d6a64318b83fb6dff64bdb1a",
      "f9042be98ca14d0fbdb7a83c49bbc88b",
      "a0d4bf9273284acba6d7f9329f5fcdb2",
      "ed65eb40f594449e89013ffb627c3255",
      "4eea156111d9413b9070666738564c85",
      "8e3b5099c0b44982b624e5b08e01a268",
      "23ededd1c4224466a72e719649fed5c9",
      "64bf51c220754cb486ba4c5b101e730a",
      "351aaeecf3744888a39738b5a4d8e0c1",
      "428ad93130a1454da931eb2b75f85265",
      "1ef7448fdde741109e306c1dc2e2afd1",
      "ad92e44c145e432080265f2bca50c433",
      "68ced3d2d3c84a51add9589c2c6d9ded",
      "8c498c53779e4e418015af3a05d2ca67",
      "82edcdb6f0794652b2728662b4ae704e",
      "bd08e03ea72544ea86b2b3da683bc131",
      "06a914c9d2334c94a5b25162f9e86770",
      "0810be5ed89842fd9b9aacca11f2d52f",
      "b88b46a55d9a43c5be0d6e6b339ba1af",
      "c891854dfa13451bb0c2eac046c889e5",
      "3e08d886c673484c83d43a4d0a5c8c74",
      "aacd1087ac474aabb28d25a247afa5d9",
      "2b48b5ddcc604cd7ab938b478d63ae1e",
      "b14efec7c2d54d7a9d40f5bd0808189c",
      "ea41b21baea34f1eb3c2cc817459c73b",
      "936f749937e541e592459d114641c046",
      "fe15f9bd9a924332b381bb5a3a9d13fc",
      "069940839dad4994ba3f0b40ecf2e85b",
      "3088ed206cb748cca2ccbf4aee891c3f",
      "b330eeca1d4547e1822e29be81d49d8f",
      "a47fe9d9e29e4867bf205f1341f53ef5",
      "dfd8b85e3b3341baa0ae3b0b3c93d0f3",
      "a06308df05ce41e591accfc3a4060684",
      "b2901a949ddf46fc949f06edfdefb29c",
      "8f39d42b936f4409a790c6341f442512",
      "7e8403014af441b88b88b2e229da3023",
      "134462100fb948e48afb1d019eeb65e1",
      "b4d21eab802f452d83eb00f3798abf7f",
      "7fbd5bd085a648dfad9bad228a160a42",
      "9cfa08f29b4e4ac0b3d3db2a5ebf17c7",
      "602b5d500ce642e895a1c76ca4ce41ec",
      "83f2fa4edf154c16b92a29f17ed1161e",
      "2984634864d34c96b1f1d9d6d1912da1",
      "c6b8d01a27414ffdbd3b7dd2ed640be3",
      "e897fdb2484943f78b2a1cf2728f85a9",
      "a7e2953dc7794cc3ac31959eb4a2c5ce",
      "3500428f23de4e009ebc6afa4e941266",
      "13187b1794224b1d99c3dd07c9c0826d",
      "67a02e2d6f30429a8e9a31ea3b17c0ea",
      "2df9765ec1ce41629dd492760a8e14e5",
      "a26f530a793e48269bd9771486e39c51",
      "b5d2803aa3604963977c0cbe0d73dfc2",
      "199bf96648be43b3b3ecca57f0ffeb82",
      "e66291dc738844d488045e582050e06b",
      "07b4d283bbcf4a00b52d8d5159937049",
      "54329306080543edbb130ee7ad20c872",
      "3639835c059045d7abe083606f2f40c5",
      "13c523a13e244b1db03bb00bf1936ba2",
      "48d3345a12aa4945b960a06f4b1d726a",
      "4ab4e6a178154e9cbe4462137042de9b",
      "7abfe3c60bc440928321315fe2a68ced",
      "71d3dd6a70614abcb85e1d06de96376a",
      "ac54b7bbdeed4c288e9bb92aca6c0480",
      "9815405d17bc4d159f9ffc4a38465b0c",
      "d950058f02b347d19cc23f0df6e48315",
      "aea04967a364457fbbec493e1b1a9c2e",
      "f349ac83fd14414aa029327464865322",
      "a06c41503f4347e7a5694ec4f8b0c55c",
      "fd55bbf6b56641348508d198495d8169",
      "83cd2cbb101d46c9a42b8886917e3c77",
      "9ec52d9803b04c6dbfe38401080a1bf3",
      "2196fa02e3a5407682847ecfe8298e3d",
      "bcd8edd8c2474d449055f9617d46030e",
      "61d473e299a94a2ab644281a81cc82cd",
      "26dc6ddbadb54d98ad420d353080113b",
      "217a696070e547ca8181783ae5950797",
      "44adac558d9a49438dfe30947f92d4ef",
      "5907180f3c4d4bedb2eea66c36a05413",
      "05024df32f464ed58cdd351de12ce755",
      "7c5f5760e30f4653b32ee79b9ed0a307",
      "6f977aeffe55426aaacc5f183fda0e89",
      "79fad1db6ee04e73b5fb7c29c36491d5",
      "302817abb3ed455792929b79051003e6",
      "ff1982e0e0a04b34b43a46df91b720bd",
      "e7bb1f650e9444cd9f2c8bfd8c725ba3",
      "b51e27d82dba43c09b3afc7c3a664c75",
      "1d6f16083a66473b883e20c1215faaf0",
      "5309e599c96b4cbea70ca80b3f92d0f9",
      "219d7045480f49228a300eb5ca355e95",
      "d37ae6195b9d41dcbc88c580f29f2878",
      "9d85bb491b574aa4a615aa9b8c195446",
      "1d39befc8f9e46319cefac506e398f37",
      "73fc1a7df57748ed8096ca5ec16689f5",
      "8b5f1d9e26dc46238fb7739efe91576a",
      "98958ad58c5c42ecb2eecfac23e47d2a",
      "e4076aa8bd2a4c03859e4dc2d1ac7769",
      "1c6014e53d64475db1962e3823790891",
      "4d9ee986c7d54c5ba7fe09ce08f692ce",
      "5929c64197a941fc94f79862d34ea054",
      "73f2d1419fb8487287391d2eba33c64f",
      "13f2c9170a1b460480ce6eac2aee06a5",
      "97dd951e29874d70a66c3a163cbe2f60",
      "532d099b134b48f8b952c55a13afac8b",
      "314c3655c3644c3b982a2f609c171899",
      "9e39bae0b03f44bfa1ba78425d19d085",
      "46b3aa33c268437aa757d35596f70ae3",
      "95378aa3dae343eeae876b29ad07191f",
      "bf783e8ced9f42c6b7396decbfe82d4a",
      "e23ed3ad8a884bca93a01c985ed840b6",
      "7cf8f3c141fc4cdd9d1c496760c27fc0",
      "94ecbc4ce0e042109d99463b9f0a6198",
      "910480124e4d4477a496894bd5890350",
      "e379cf13b6ca464fbccce87df566ad9a",
      "3aaaa4ed6c5b420c93490a04d5d1a168",
      "cf56712a053345858bc1dd13d0d34e58",
      "ba3f52149cf54c98a290d27221e6033e",
      "8e8672d64c4a41c585f42df05a258526",
      "a32b78261aa748b682a8978f01ef14f5",
      "18cd206bc3144cf69206c2f05fb6c4d9",
      "18b11e2a39a8463c9e1443c77ef32b05",
      "eb8931b911ab4789b44d5b192d5adb25",
      "120c6a76d352492dbefac93d7a094e39",
      "052dc18f27cd43f482b4ac3c79764d28",
      "babfc151a69f4b1fa8f081a22da33ff7",
      "555bc3a9f5e0459b9e96b06ba20106fd",
      "d818ff28277942b888528a7f8935374b",
      "df65e11a1d1a4cceb8ea44399f17690a",
      "44c7e305d91b4e5a98a40aec8e2eacc4",
      "515abdbf1f744b3f9f5a436f1815b795",
      "57759386c32548d8a47aaef00dd80367",
      "c7ffbae192034c6485900c91482d6c8e",
      "c53aded54c98444da4da286ed4d20639",
      "925bd4c23c9d4b0b9c9d48c30738269e",
      "7d685a255e544c28af2745d409ce1474",
      "15a20b80c784456eabbf71f85c5fd2e2",
      "08f5080b6d1648b8a15b8bd1c39e6a8e",
      "7f214659433e455baf6c4b0394e8594d",
      "800a164c86764c5691dc54ee0d391dbf",
      "4755b34ea43b4331aa9c7446bba9fed1",
      "ecb24ad8a26e43099935ab6afde8557b",
      "a45bf64964ea48cd9b203fe5d3b91bb7",
      "b930a9d824da4396b3e57dbf2f383be0",
      "7a3b37907374484fbaf26e6127a5b5cf",
      "0cd8b52097e346dca89eefe673231a2e",
      "44a831fea9ab4528b6f566533e66c761",
      "3e7498c7cfad485681d5c624dd7b3533",
      "d8094107b2ff4a3f8566d2a37e178f48",
      "b4aec306a80b404cbc02e275a376eef3",
      "c7053a3b821443e8b439e8a199d3a282",
      "85adf7a80604413bbde66953bfef40f7",
      "de71dfadbd794b8aa63d94355981bf1b",
      "bb98b349307445a1b2aef70f1c29a67c",
      "878c4bbbcb8f402f8d88e06d469ef6a7",
      "dfb7138f608d427f8fb3c3711cb72a55",
      "b9fccd26fb4942ed9426100e176486d4",
      "e3ec4ce2c3ea4f4b820ad60338e01464",
      "544e2e7df45945e2a3440f4b69d112fd",
      "0968c1c3e6714bbb8104a83cd0b28b8d",
      "07c8979cc4f842ba8fa30ac82bd53dba",
      "0cd0715a21d64632bdd612a918de251a",
      "9babbedcaa1344debb1566906a2a944c",
      "ae6989698d2343349499598b3ecf58cb",
      "9220148fe24a4872aa75e3f5a88c7ff1",
      "657cbfbd6db1430b826b6189ee555dd6",
      "b6597ec46c124b96822282acad9a34f8",
      "987c95ecfadf41c1a26bab59f1d50d6b",
      "b8feb7392f3345ca98a69d5a80da708c",
      "3f2a6f97f8e841298afeb1f99e77d2b5",
      "90958bda7dc94db8bd9df4159cd55990",
      "49a1e7905d6f4fa39271f5c2eeb8ca2d",
      "a14810588c9b4ab8b7626efb0cb3645f",
      "510ecb775d0244abbac32431076e8c18",
      "8ce1a53b1e4b4193b1f766cbe6437877",
      "7bacd96695cf4e8187abe8db5d5c4031",
      "35757ae46c2e41569d1dad9e4e861dd4",
      "fb0b1928e4b340eb86ac340cf91fbe3a",
      "7143e4d76f414c818c97286f339e9b44",
      "b9605a6ab6d64fa2b6d77cebb47cc9fd",
      "4755a560e6ac40a1b5db1bf5398096c5",
      "6e47c7ecca2c4ec29a6febd0d61eda73",
      "0bc8b56133174248bae274e1fe1bf67a",
      "a1634b07e72744fa844a03fc7424edb4",
      "60357cafb7a14a7085e9fbcdbc7b29c8",
      "9b4e4fb9c6434e4fa481bb3831f44079",
      "e12ab67590864285ac0f9ee00ce8c458",
      "96c40a30fc1743eb9f84422fcb666608",
      "f9ed9791999948f7811f8cf14e9ee05c",
      "f6598e0547e44ff9a17c689c7110d01e",
      "b64ff2fc90fc414ea76587fb153542a7",
      "a38b95b6a0d443aa921c4547698183ef",
      "cccd734336a84f69adca65107e6cab67",
      "11531d0d9c47435e8112ffa25958890b",
      "a12369b71e3942ccb6a09d54c9dd31d5",
      "05f4cc6d66d34b88badb8959ee93fd03",
      "0b3963c0c6b84458b8799e7924477a7c",
      "c781d9dd386645e7981296beae51f070",
      "0adf14937df644cb9e542a1690cfb2e6",
      "7b1b8b401ddc4bf9b7cd5326b39cfb2d",
      "f5721f6c26834159a3870b71ad7bf4f8",
      "f5b2e8acd06a41bb8564a4b9a60624d4",
      "1db204edb154473da72448b7e47596ff",
      "c372e93c20564b5399ddb24a25ba4622",
      "c0eee6fec0004f36848be0a0d7764483",
      "9107040374e94a4ea7ce893289bdde90",
      "39edd8fe7cde46ab85a4244b8f54d5f2",
      "6fd6243cf6394446b55d5a4c4910792a",
      "331952bbdbfe42a38ac45ebdbdfcbcd5",
      "f35df4642e03476dacc7378bd11b7bf1",
      "994df93e19b143d4aedc4ee89d3a6553",
      "49779d7ddd2244e395370c9594036650",
      "1b70f45851fb4aec9acf732bda2cd023",
      "e6321cdbc342484c9d4114eefaa4179a",
      "ba78d1cd792e4f0e973c15c32447d743",
      "c07f0fc959964b1684cbafa7655edbd4",
      "640048f74b9041e1b22f8ec1bafb20cb",
      "ed8bf8f2e571432ba0e5c1cf4c3a2d45",
      "332c775427e0473ba7f46c9c029f54c3",
      "d842059b4ae342c1b9e38efea7005a57",
      "e2d3afbcb19248dda74cd869af6af6f8",
      "a2102f7c886b4533b9d8a7fc69b596b6",
      "a9f84a951c0548978935fd1b81286504",
      "208d41ca45184ab3bfe97e5e857a14a5",
      "ca7c719476f04db7a740dcbeebccfbac",
      "02848d04eb7a42c8b1ad68cfd0aa5401",
      "04a6cec8d6c84128b536e3ce4809983c",
      "e508829f4b9d48888bc3b33e578bafc1",
      "3e6cf706a53641ff9c26240e8495cee0",
      "d65bc35dd7d54381a59475e70a58ed30",
      "9565246ae57447c0bb4905c630fbbec8",
      "582b5891df2b4943a15953f1ceea92e3",
      "e299174b96734f4e801d10b70999c4e7",
      "e1ccffbd569e48d8bebc92a6abef367b",
      "138312c4506f41fab3ea3abcdd8683bc",
      "47f508e144fa4cf0981b4b36f2080df6",
      "cd292a8893b94a87a8b7f15f59907d6f",
      "8405663ab4c644caaa2d99b5e24365f1",
      "44ba7c44dcdf4caa86d27f8037bab318",
      "4e33629c232f4ddc9aecc75baece5eb9",
      "229e698927234ad3a522d656a4352419",
      "2bb353264fe14c94936dd3d7bda0d2d1",
      "3932911f22564090ad74318c8724726c",
      "16282b0ad8c54b0dbe4978e8b47bf0eb",
      "d883b6f4337b42ffaf2667051987b36e",
      "32b4a165ba444733b8f0234dab837c04",
      "5a208d4b5a6b46db943866eb76809deb",
      "36a74dab8ec940588ee8c0e0ec66011c",
      "5410ea964139425488bcf37986a592f5",
      "4630fa7a63eb46b0a0f64c23ace9f8b3",
      "c4d2b7c1ec3f4beeb3677bc1f2ba3591",
      "b70253f947f143d98a50298123e3f93a",
      "12765ea12ead4b3691bb415136797ff7",
      "8d2f948783034dcfb16159a97e28a245",
      "94ce91888fac433984b750990b0f9541",
      "31761ee1f43947d1a2009390199f73ca",
      "83a04a6873914c588f7316834c9d7087",
      "e1ab9c3d2ed447038827fd496bea2401",
      "604b8821953d4ee58d63eecf21630ccd",
      "dbeb3ea809494253a72281d9a142ab62",
      "ee866d4b63304cff9802363e821c6642",
      "edaee4f8a043476692a1159d73a25bdb",
      "9a2440694c5e49c4b23d610785c6b0c0",
      "3a481111f6944d63b239afa912bfe55f",
      "efdb665aa1f8436e8b1f6f6a209a34e8",
      "3a5150bdca014052bbf80fd756f715c4",
      "5c5689d3f7f94ab9afd180dc348b2cee",
      "c56ebeba4a7b4134a71cf743496ee4a0",
      "f992c067acb944949edbbd1061e9a955",
      "ccff34f4a2044c559ea9c0bda24b1128",
      "e8a96c6f1a82468ca6ae60ebdcee9305",
      "c9713d44e66c4f1d8557fa5334af8835",
      "c19d0f76adbd42d7838422d01b9027f4",
      "06f23477e66043eeb13f3fdd318bc34d",
      "9389ab2bd05c408ebb320b2242dfb73c",
      "4ea25b70f2fc470dae92ecb0b6391ad4",
      "5ff9b7a232324267ac596dc11a5e7c44",
      "60c6f3ba62274c83a59237e7e11fecf4",
      "f80362d4fbc84eb1870d42a0a7c3510d",
      "cae3745a5fc642059aafa312e765fe0b",
      "cf452a77ab8e4a5885ebe2ada8be497e",
      "8b9af286235642f78a807c11c7e8c362",
      "5122392e26e74f87bae01ddc3c93b410",
      "77ce8ce6486c4297ad03a750ea9fab3d",
      "1e14e5351eab4503b8fac880c094bdf4",
      "05f53c162c8d4a87b64fe3ebbe046089",
      "641c0b37e0e9450a8b129aab43806909",
      "95289d127d484a29ab5cdfae8a37b921",
      "4d22089a1de6442bb12476adf89fb2e4",
      "89a733fdbd254065a2500e621c18775b",
      "72739c056dab40f1bdea3feb660ae72c",
      "32631aa83b084ecca23185209cb3fb6e",
      "507bd911160c47dfb95d4a42ac717d16",
      "61087d628d4944f3844ba22f5a873cd6",
      "c772caf1235346e4b8a492b231d60b16",
      "1d34caf878cb4d5dbd18cbb01c208a8c",
      "9abce017e33747a58ba2b9d461bc6217",
      "04cd6cfa1d384fbe94d89fcbce3cf34e",
      "a9fc1ed72e7b4dd081c38155c0c48c93",
      "2bc98920da9e44d78a4cfcb3281ea843",
      "4fb3a597a60f4bf2b7e3894b7a118900",
      "12e656092f284933802fc7e6af8e1d9d",
      "41f9343b547b46529aa89948dc0e02b9",
      "95c0e50559814a5fa7c2010572ba3941",
      "2d18b55e90614fc9bfd4acb7912a974b",
      "3c6ba592dfd243fca06937830e0adddc",
      "0c0167ca10344db19acc654bc68720ed",
      "e6a09076b74a4d2fa3a9451858c2234a",
      "0b1487eb55b446a79a77a8e93cc204e5",
      "8f3b2bf2da0148829250107b06b315a5",
      "d00b670f723d4d3db12f752c4b9ea09d",
      "e76a711f88854e73ab7edd7eca978dde",
      "7db2d3b053284e92a47ed22c38b657c9",
      "bc2b4c2b6ec84287ba7c9de5653ee9c9",
      "219d6dbbdf7e4af887401f0dc8cb6f24",
      "5e8489eba9194abbb87bee78a6316855",
      "093a4faa33064aa58d057d2fdc0f77c3",
      "51847bf6de7c48c19c8f04c161dc9a78",
      "eda395b690254cca9b0c76b88d937cc2",
      "10e426bea887432fad9e3ac1eac1492c",
      "8cf7b49fb4ce4d2694edbbf8787cd648",
      "abcfa33b57e24151ba2cebedece33d8d",
      "bfd1fa845fd042a5b2b7f0e35607bfcf",
      "ef8ce2ddacc4470cb167f29c20f387fd",
      "6d8897796ad34ab0b3becd3693293c0b",
      "a63be4d4dddc47d1b7c049681bcba477",
      "e519c19e32e743d3af9b9ef45ef9b26e",
      "af708feab28e40aabbf7dfa22f62c359",
      "355eec18e6984fde848a6f5cbf0421d2",
      "c8619ba1e54a4bafa5b4f6ffdc500063",
      "c562b56c86844c68bcb36573fb2cd036",
      "c631865543214b9ca689dd39a08ce42c",
      "38036d4120264ee1bd16899e88ca7d2c",
      "e07346fa68cd4a1cbe80508a755caa30",
      "bd7fb234f6aa4c2e9287e135734c6ebc",
      "91606189c3834772b7280c4443a6bced",
      "a55294bff42c47379f1941d7c416579a",
      "23a50e3b13604767b7e4193a69bff018",
      "c7d0f7c518ba42f09058d1e0e9472a00",
      "e6f5ff0fdf7f47669857079c8cb0ded9",
      "7c2fb6564f3a43f3b0e0585cd3042862",
      "25e5817c81484da8bf0f6fa1f83bffd0",
      "299445be0dd14e4f8470a2103da40e86",
      "e659b1912e8f4c85b80f890a72d0281a",
      "e84de881e82e44e9a44500db0d85083d",
      "1bfdb64a1c25475998449e7b8fb1bc80",
      "3db458ffd32a493fb85e04d004151291",
      "5b765612f04c4aefbd446db4a1e1368c",
      "fea7b0dea9a343a7a304249e2ad53255",
      "54aedb1ace4d44fb9a8920f1c4533458",
      "4b53e96ef5ac42dca09ce27a60991c6c",
      "c97896d4ce5c4d98acfedbda517bf983",
      "d7bb3c1249754b94ac600ed671d1f21b",
      "5b5b5e47d0b34084adb5819c8346ee11",
      "b473fb28ed1a43c99b08ec7f8c483726",
      "9d65bad226b34fffb65184805c4837a3",
      "3c199b6825284d17aa3143a92d76a18d",
      "dc2dc4c2500e419da2c32169a74936b1",
      "e9064fd8e04c423a86f4c078113bda35",
      "22e12fe6686043f4a1e441a76f25b6f9",
      "0c8bbf1e67944464a87c5a05caa05d2c",
      "7e57add6d5254bb3b0342a036e4d5e66",
      "2237dd5f373d4e20b44a53b8f1e1b634",
      "0c504423c9964e45a58516bcc57a2b9f",
      "ad9eede4b600464ea288580e1dda6d80",
      "7afe500fbae243a385afa88abcd082ea",
      "ed523e5ec61b41a9bbd50398430bb3a8",
      "f9c30da9262c4b4d9593318741d35c88",
      "7c787613bab244cf82666e830c1ff02b",
      "11826de8175143c09bc8b536d538a508",
      "9d63679ea80f4699b66d371449b8ca29",
      "25a0e88f73064777825f9e6c82660157",
      "c52947f56c3044f1b9b37f34474a5f70",
      "f13bc0b466d84e3aa8b40e65b92d3569",
      "0ea68195ab334721a1c127f699e8b71e",
      "0e23561288e44583806d618eb43d05e0",
      "49f336a03f7e4c2ab11df87b7ef540d1",
      "f017d4b553624a4ebe53686960546a91",
      "dadf60a5bc6942479f1ffbaab9dd7120",
      "c67d9867709b43f099c994a1b928b183",
      "06d522ada550428892af223325b8efd4",
      "bd8b80c28dfa4e5ea1586e797d5630b5",
      "2454416710434b999c2f0e87dd2da3bc",
      "81be66f0b9ea455ebc010aef95e0b99d",
      "b26c654ec3a3454dba80d3775a2761f3",
      "cf84034b342540dd988ab867d7dd3575",
      "f6c21e583c1a42dab13820d2cb9ca52e",
      "96af57e99e5d4159811e61d22b76b866",
      "9093e83cb18c4bd096f8a944a4740f8f",
      "7d58aefb52f741c48d8b9ec789139fe5",
      "f02787a4946c40719a53b3d761370672",
      "b31be898bcf04ee0a1f98c33ae936a41",
      "70c6d1dd451e41159fee9471322bd2ec",
      "0bd5a5581aef4c20adb0057c0367aa15",
      "a2e065bb7e62496f876c1069a5ab385e",
      "59e9c3a0a9d64ef5a56ac294754e1302"
     ]
    },
    "id": "mjsS2x23k4yB",
    "outputId": "49abf136-ec90-423e-c33f-2cd10ad2509a"
   },
   "outputs": [],
   "source": [
    "train_dataset = load_dataset(\"Woleek/EmoSet-118K\", split=\"train\")\n",
    "test_dataset = load_dataset(\"Woleek/EmoSet-118K\", split=\"test\")\n",
    "val_dataset = load_dataset(\"Woleek/EmoSet-118K\", split=\"val\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KlekW5vSiHZc"
   },
   "source": [
    "In terms of labels:\n",
    "- Amusement = 0\n",
    "- Awe = 1\n",
    "- Contentment = 2\n",
    "- Excitement = 3\n",
    "- Anger = 4\n",
    "- Disgust = 5\n",
    "- Fear = 6\n",
    "- Saddness = 7\n",
    "\n",
    "We want to turn them into:\n",
    "- Amusement = 0\n",
    "- Awe = 0\n",
    "- Contentment = 0\n",
    "- Excitement = 0\n",
    "- Anger = 1\n",
    "- Disgust = 1\n",
    "- Fear = 1\n",
    "- Saddness = 1\n",
    "WHERE 0 = Positive, 1 = Negative!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vor0FviSwt6D"
   },
   "source": [
    "Transforming EmoSet data into PyTorch compatible data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VTiwbTs9YMjx"
   },
   "source": [
    "Emoset Authors: For 'train' phase: Random resize crop to 224, random horizontal flip, conversion to tensor, and normalization.\n",
    "For 'val' and 'test' phases: Resize to 224, center crop to 224, conversion to tensor, and normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eScvpsTbYURK"
   },
   "source": [
    "## Changes made below\n",
    "removed color jitter from train (actually probably important info re emotion)\n",
    "removed attributes in transform (for now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WfPFVqTtX1In"
   },
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    #THESE FIRST COUPLE PARTS are essentially doing \"data augmentation\" without using up any memory\n",
    "    transforms.RandomResizedCrop(224),  # Randomly crop and resize to 224x224\n",
    "    #transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1), # Randomly adjust color properties\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "# Define transform\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XCx-pICZbJWv"
   },
   "source": [
    "The above are transformers, or \"pre-processing steps\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ruYLHHs9X4ay"
   },
   "outputs": [],
   "source": [
    "# define for per-sample transformations. use wrapper function bc hugging face fx only allows one argumnet (batch) but want to load different transforms depending on the set\n",
    "\n",
    "def transform_wrapper(transforms=train_transform):\n",
    "    def transform_batch(batch):\n",
    "        pixel_values = [transforms(img.convert(\"RGB\")) for img in batch[\"image\"]]\n",
    "        labels = [0 if x <= 3 else 1 for x in batch[\"label\"]]\n",
    "        return {\"pixel_values\": pixel_values, \"labels\": labels}\n",
    "    return transform_batch\n",
    "\n",
    "\n",
    "# turn single values into tensors of pixel values ?\n",
    "def collate_fn(batch):\n",
    "    # Stack pixel tensors\n",
    "    pixel_values = torch.stack([item[\"pixel_values\"] for item in batch])\n",
    "    labels = torch.tensor([item[\"labels\"] for item in batch], dtype=torch.long)\n",
    "\n",
    "    return {\n",
    "        \"pixel_values\": pixel_values,\n",
    "        \"labels\": labels}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ab69HnPjwy6L"
   },
   "source": [
    "Creating a way to transform single values into tensors!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bVEhuMy1bwn1"
   },
   "source": [
    "#Part 1: Training EmoSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pa7tHX4sb4_O"
   },
   "source": [
    "##Transfer Learning from ImageNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jyYsQqXzxN8q"
   },
   "source": [
    "NOW we finally load each set using the prior defined functions in order to create batches of info usable by PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3B4qJ6KvlI-w"
   },
   "outputs": [],
   "source": [
    "# Create DataLoader\n",
    "test_dataset.set_transform(transform_wrapper(transforms=val_transform))\n",
    "train_dataset.set_transform(transform_wrapper(transforms=train_transform))\n",
    "val_dataset.set_transform(transform_wrapper(transforms=val_transform))\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False, collate_fn=collate_fn)\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=False, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=256, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nmad7JaexYZX"
   },
   "source": [
    "Functions to check accuracy of all our EmoSet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_UItQwdPayl4"
   },
   "outputs": [],
   "source": [
    "def check_accuracy_EmoSet(loader, model, N=256):\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            x = batch['pixel_values']\n",
    "            y = batch['labels']\n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "            scores = model(x)\n",
    "            _, preds = scores.max(1)\n",
    "            num_correct += (preds == y).sum()\n",
    "            num_samples += preds.size(0)\n",
    "        acc = float(num_correct) / num_samples\n",
    "        print('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EBuKb3NJxiZ9"
   },
   "source": [
    "This will be used to TRAIN our created models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hB-Va2WKaHxs"
   },
   "outputs": [],
   "source": [
    "\n",
    "print_every = 100\n",
    "\n",
    "def train_EmoSet(train_load, val_load, model, optimizer,epochs=1):\n",
    "    \"\"\"\n",
    "    Train a model on CIFAR-10 using the PyTorch Module API.\n",
    "\n",
    "    Inputs:\n",
    "    - train_load: the train_loader\n",
    "    - val_load: the val_loader\n",
    "    - model: A PyTorch Module giving the model to train.\n",
    "    - optimizer: An Optimizer object we will use to train the model\n",
    "    - epochs: (Optional) A Python integer giving the number of epochs to train for\n",
    "\n",
    "    Returns: Nothing, but prints model accuracies during training.\n",
    "    \"\"\"\n",
    "    model = model.to(device=device)  # move the model parameters to CPU/GPU\n",
    "    for e in range(epochs):\n",
    "        for t, batch in enumerate(train_load):\n",
    "            x = batch['pixel_values']\n",
    "            y = batch['labels']\n",
    "            model.train()  # put model to training mode\n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "\n",
    "            scores = model(x)\n",
    "            loss = F.cross_entropy(scores, y)\n",
    "\n",
    "            # Zero out all of the gradients for the variables which the optimizer\n",
    "            # will update.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # This is the backwards pass: compute the gradient of the loss with\n",
    "            # respect to each  parameter of the model.\n",
    "            loss.backward()\n",
    "\n",
    "            # Actually update the parameters of the model using the gradients\n",
    "            # computed by the backwards pass.\n",
    "            optimizer.step()\n",
    "\n",
    "            if t % print_every == 0:\n",
    "                print('Iteration %d, loss = %.4f' % (t, loss.item()))\n",
    "                check_accuracy_EmoSet(val_loader, model)\n",
    "                print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zpgBwBlOyWn1"
   },
   "source": [
    "Lets try an EmoResNet model WITHOUT adding variables outside of just the image and label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bTedj_V6g1cw"
   },
   "outputs": [],
   "source": [
    "class EmoResNet(nn.Module):\n",
    "  def __init__(self, num_classes=2, train_base_model=False):\n",
    "      super().__init__()\n",
    "      weights = ResNet50_Weights.IMAGENET1K_V1\n",
    "      base = resnet50(weights=weights)\n",
    "\n",
    "      # optionally train or freeze base model parameters\n",
    "      for param in base.parameters():\n",
    "            param.requires_grad = train_base_model\n",
    "\n",
    "      self.base_model = nn.Sequential(*list(base.children()))[:-2] # base.children are all the modules within resent. remove avg pool and fc\n",
    "\n",
    "      # add new model\n",
    "      self.new_model = nn.Sequential(\n",
    "        nn.Conv2d(2048, 512, kernel_size=5, padding=1),\n",
    "        nn.BatchNorm2d(512),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(2),\n",
    "\n",
    "        nn.Conv2d(512, 128, kernel_size=3, padding=1),\n",
    "        nn.BatchNorm2d(128),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(p=0.4),\n",
    "        nn.MaxPool2d(2),\n",
    "\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(128, 8),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.4),\n",
    "        nn.Linear(8, num_classes)\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.base_model(x)\n",
    "    x = self.new_model(x)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wKZ1xJ1NjnVP",
    "outputId": "77c3e654-802c-47c4-c520-2a38f501b7c9"
   },
   "outputs": [],
   "source": [
    "model = EmoResNet(num_classes=2, train_base_model=True)\n",
    "\n",
    "# # use adam optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3) # authors use default lr and adam\n",
    "\n",
    "#train_EmoSet(train_loader, val_loader, model, optimizer, epochs=3)\n",
    "train_EmoSet(train_loader, val_loader, model, optimizer, epochs=5)\n",
    "\n",
    "# # Suppose your model variable is called `model`\n",
    "save_path = FOLDERNAME + \"finetuned_modelEmoSet.pth\"\n",
    "\n",
    "# # Recommended: save state_dict (weights only)\n",
    "torch.save(model.state_dict(), save_path)\n",
    "print(f\"Model saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sDJrkRig0PXu"
   },
   "source": [
    "Here, we run and save the model before testing it on our Test Set! We can then load it whenever we want and run it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PxorIFdA11c1",
    "outputId": "23764b5d-d2b6-46e9-bb61-3342bf885f92"
   },
   "outputs": [],
   "source": [
    "# Recreate the same model architecture\n",
    "model = EmoResNet(num_classes=2, train_base_model=True)\n",
    "save_path = FOLDERNAME + \"finetuned_modelEmoSet.pth\"\n",
    "\n",
    "# Load weights\n",
    "model.load_state_dict(torch.load(save_path, map_location=device))\n",
    "model.to(device)\n",
    "model.eval()  # important: sets dropout/batchnorm to inference mode\n",
    "\n",
    "print(\"Model loaded and ready for inference.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hlCDX35sOJAz",
    "outputId": "e3fa032f-39c4-4f7d-a91c-dcc0f57fcdc6"
   },
   "outputs": [],
   "source": [
    "print(\"Checking accuracy on test set:\")\n",
    "check_accuracy_EmoSet(test_loader, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LtDtWXWPv1o2"
   },
   "source": [
    "#Step 2: VALIDATE on NAPS/OASIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eeqq29TIclPA"
   },
   "source": [
    "##Pull in and set up Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bqE3MfgMv4Se"
   },
   "outputs": [],
   "source": [
    "def load_file(filepath,file_types):\n",
    "\n",
    "  #Creating a list that returns all filepaths\n",
    "  all_files = []\n",
    "\n",
    "  #os.walk goes through each file and each directory. recursively joins all\n",
    "  for root,dirs,files in os.walk(filepath):\n",
    "    for file in files:\n",
    "      if any(file.endswith(ft) for ft in file_types):\n",
    "        all_files.append(os.path.join(root,file))\n",
    "\n",
    "  return all_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qghQZGPav9QB"
   },
   "outputs": [],
   "source": [
    "from pickle import FALSE\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, img_list, image_name, valence, arousal, source, augmentations = None, transform = None, val_continuous = False):\n",
    "        super(MyDataset, self).__init__()\n",
    "        self.img_list = img_list\n",
    "        self.augmentations = augmentations\n",
    "        self.image_name = image_name\n",
    "        self.transform = transform\n",
    "        self.arousal = arousal\n",
    "        self.valence = valence\n",
    "        self.source = source\n",
    "        self.continuous = val_continuous\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.img_list[idx]\n",
    "\n",
    "        \"\"\"\n",
    "        Meant for Binary Valence\n",
    "        \"\"\"\n",
    "        valence_val = None\n",
    "        if self.continuous:\n",
    "          \"\"\"\n",
    "          Meant for Continuous Valence\n",
    "          \"\"\"\n",
    "          valence_val = self.valence[idx]\n",
    "        else:\n",
    "          source = self.source[idx]\n",
    "          cutoff = 7\n",
    "          if source == \"NAPS\" or source == \"Explore\":\n",
    "            cutoff = 9\n",
    "          #for the NAPS data, rating goes from extremely negative to extremely positive!\n",
    "          #For OASIS data, 4 = Neutral, 1 is negative, and 7 is positive\n",
    "          if self.valence[idx] < float(cutoff/2) and source == \"Explore\":\n",
    "            valence_val = 0\n",
    "          elif self.valence[idx] >= float(cutoff/2) and source == \"Explore\":\n",
    "            valence_val = 1\n",
    "          elif self.valence[idx] < float(cutoff/2):\n",
    "            valence_val = 1\n",
    "          else:\n",
    "            valence_val = 0\n",
    "\n",
    "        arousal_val = self.arousal[idx]\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "          pixel_values = self.transform(image)\n",
    "\n",
    "        return {\"pixel_values\": pixel_values, \"valence\": valence_val, \"arousal\": arousal_val, \"image_path\": image_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N_kgwq1PS6Co"
   },
   "outputs": [],
   "source": [
    "# Define custom collate_fn to ignore non-tensor fields / tconvert arrays to tensors\n",
    "def collate_fn_NAPSOASIS(batch): # batch is a list of dictionaries\n",
    "    pixel_values = torch.stack([item[\"pixel_values\"] for item in batch])\n",
    "    valence = torch.tensor([item[\"valence\"] for item in batch])\n",
    "    arousal = torch.tensor([item[\"arousal\"] for item in batch])\n",
    "\n",
    "    return {\"pixel_values\": pixel_values, \"valence\": valence, \"arousal\": arousal}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B9aBpwV0XvTA"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def remove_samples(min,max,df, remove):\n",
    "  \"\"\"\n",
    "    set is the set of data\n",
    "    min is minimum of where to remove\n",
    "    max is maximum of where to remove\n",
    "    remove is ratio of how much to remove\n",
    "  \"\"\"\n",
    "\n",
    "  copy_df = df[(df[\"All_ValenceM\"]>=min) & (df[\"All_ValenceM\"]<=max)]\n",
    "  num_in_between = copy_df.shape[0]\n",
    "  to_remove = int(num_in_between*remove)\n",
    "\n",
    "  indexes = list(copy_df.index.values)\n",
    "  random_items = random.sample(indexes, to_remove)\n",
    "\n",
    "  df_cleaned = df.drop(df.index[random_items]).reset_index(drop=True)\n",
    "\n",
    "  return df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nbHIKxr2V8Kq"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "#PULL IN ALL FILES:\n",
    "NAPS_path = FOLDERNAME + \"NAPS_data\"\n",
    "OASIS_path  = FOLDERNAME + \"OASIS_data\"\n",
    "\n",
    "mapping_path_NAPS = load_file(NAPS_path,[\"table.csv\"])[0]\n",
    "NAPS_data = pd.DataFrame(pd.read_csv(mapping_path_NAPS))\n",
    "NAPS_data[\"source\"] = \"NAPS\"\n",
    "\n",
    "#HERE, let us attempt to CUT DOWN on some \"neutral\" values - AKA, remove 50% of samples between 4-5\n",
    "NAPS_resampled = remove_samples(4,5,NAPS_data, 0)\n",
    "\n",
    "arousal_mean = np.mean(NAPS_resampled[\"All_ArousalM\"])\n",
    "arousal_stdv = np.std(NAPS_resampled[\"All_ArousalM\"])\n",
    "NAPS_resampled[\"All_ArousalM\"] = (NAPS_resampled[\"All_ArousalM\"] - arousal_mean)/(arousal_stdv) # normalizing\n",
    "\n",
    "mapping_path_OASIS = load_file(OASIS_path,[\".csv\"])\n",
    "OASIS_data = pd.DataFrame(pd.read_csv(mapping_path_OASIS[0]))\n",
    "OASIS_data = OASIS_data.rename(columns = {'Valence_mean':'All_ValenceM', 'Arousal_mean':'All_ArousalM','Theme': 'ID'})\n",
    "OASIS_data[\"source\"] = \"OASIS\"\n",
    "\n",
    "OASIS_resampled = remove_samples(3,4,OASIS_data, 0)\n",
    "\n",
    "arousal_mean = np.mean(OASIS_resampled[\"All_ArousalM\"])\n",
    "arousal_stdv = np.std(OASIS_resampled[\"All_ArousalM\"])\n",
    "OASIS_resampled[\"All_ArousalM\"] = (OASIS_resampled[\"All_ArousalM\"] - arousal_mean)/(arousal_stdv) # normalizing\n",
    "\n",
    "# actually load in images\n",
    "NAPS_OASIS_data = pd.concat([NAPS_resampled, OASIS_resampled], ignore_index=True, axis = 0) # combines df\n",
    "\n",
    "NAPS_image_path = NAPS_path + \"/NAPS_H\"\n",
    "list_of_paths1 = load_file(NAPS_image_path,[\".jpg\"])\n",
    "list_of_paths2 = (load_file(OASIS_path,[\".jpg\"]))\n",
    "list_of_paths = list_of_paths1+list_of_paths2\n",
    "image_data = pd.DataFrame(list_of_paths, columns = [\"image_path\"])\n",
    "image_data[\"image\"] = image_data[\"image_path\"].apply(lambda x: x.split(\"/\")[-1].split(\".\")[-2])\n",
    "\n",
    "# merge image data with labels\n",
    "image_data_final = pd.merge(image_data, NAPS_OASIS_data, left_on = \"image\", right_on = \"ID\", how = \"inner\")\n",
    "image_data_final = image_data_final.drop([\"image\"], axis = 1)\n",
    "\n",
    "def normalize_valence():\n",
    "  arousal_mean_OASIS = np.mean(OASIS_data[\"All_ArousalM\"])\n",
    "  arousal_stdv_OASIS = np.std(OASIS_data[\"All_ArousalM\"])\n",
    "  OASIS_data[\"All_ArousalM\"] = (OASIS_data[\"All_ArousalM\"] - arousal_mean_OASIS)/(arousal_stdv_OASIS) # normalizing\n",
    "  arousal_mean_NAPS = np.mean(NAPS_data[\"All_ArousalM\"])\n",
    "  arousal_stdv_NAPS = np.std(NAPS_data[\"All_ArousalM\"])\n",
    "  NAPS_data[\"All_ArousalM\"] = (NAPS_data[\"All_ArousalM\"] - arousal_mean_NAPS)/(arousal_stdv_NAPS) # normalizing\n",
    "\n",
    "  valence_mean_NAPS = np.mean(NAPS_data[\"All_ValenceM\"])\n",
    "  valence_stdv_NAPS = np.std(NAPS_data[\"All_ValenceM\"])\n",
    "  NAPS_data[\"All_ValenceM\"] = (NAPS_data[\"All_ValenceM\"] - valence_mean_NAPS)/(valence_stdv_NAPS) # normalizing\n",
    "  valence_mean_OASIS = np.mean(OASIS_data[\"All_ValenceM\"])\n",
    "  valence_stdv_OASIS = np.std(OASIS_data[\"All_ValenceM\"])\n",
    "  OASIS_data[\"All_ValenceM\"] = (OASIS_data[\"All_ValenceM\"] - valence_mean_OASIS)/(valence_stdv_OASIS) # normalizing\n",
    "\n",
    "  # actually load in images\n",
    "  NAPS_OASIS_data = pd.concat([NAPS_data, OASIS_data], ignore_index=True, axis = 0) # combines df\n",
    "\n",
    "  # merge image data with labels\n",
    "  image_data_final = pd.merge(image_data, NAPS_OASIS_data, left_on = \"image\", right_on = \"ID\", how = \"inner\")\n",
    "  image_data_final = image_data_final.drop([\"image\"], axis = 1)\n",
    "\n",
    "  return image_data_final, valence_mean_NAPS, valence_stdv_NAPS, arousal_mean_NAPS, arousal_stdv_NAPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EXgwLgZhEx1a"
   },
   "outputs": [],
   "source": [
    "dataset_val = MyDataset(img_list=image_data_final[\"image_path\"],image_name = image_data_final[\"ID\"], valence = image_data_final['All_ValenceM'], arousal = image_data_final['All_ArousalM'], source = image_data_final['source'], transform=val_transform, val_continuous = False)\n",
    "val_loader_NAPS = DataLoader(dataset_val, batch_size=32, shuffle=False, collate_fn = collate_fn_NAPSOASIS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pTkwi1JI24Mn"
   },
   "source": [
    "##Adapted checks for NAPS/OASIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rowPNX7c26gp"
   },
   "outputs": [],
   "source": [
    "def check_accuracy_NAPSOASISVAL(loader, model, N=256):\n",
    "\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            x = batch['pixel_values'].to(device)\n",
    "            y_valence = batch[\"valence\"].to(device=device, dtype=torch.long)\n",
    "\n",
    "            valence_pred = model(x)\n",
    "\n",
    "            # emotion accuracy\n",
    "            _, preds = valence_pred.max(1)\n",
    "            num_correct += (preds == y_valence).sum()\n",
    "            num_samples += preds.size(0)\n",
    "\n",
    "            # # bright acc\n",
    "            # mae_brightness += F.l1_loss(bright_pred, y_bright, reduction='sum').item()\n",
    "            # #mae_brightness += torch.mean(torch.abs(bright_pred - y_bright))\n",
    "\n",
    "    acc = 100.0 * num_correct / num_samples\n",
    "\n",
    "    print(f'Valence Accuracy: {num_correct}/{num_samples} ({acc:.2f}%) ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DKD7frnyyoR6"
   },
   "source": [
    "Great, now we have data loaded in! We need to attempt to use this as a VALIDATION SET - how do we do this? Let us test on the Original EmoSet Model, but this time, send in NAPS/OASIS validation set to see transferability!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GIaQ1IZczwpT",
    "outputId": "6d800736-59a0-45b5-b60b-ebba8fb6c263"
   },
   "outputs": [],
   "source": [
    "print(\"Checking accuracy on NAPS/OASIS set:\")\n",
    "check_accuracy_NAPSOASISVAL(val_loader_NAPS, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vKWkZB1cd5iU"
   },
   "source": [
    "#IMPROVE model for NAPS/OASIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N97zMOgqdU3H"
   },
   "outputs": [],
   "source": [
    "class NapOasEmoResNet(nn.Module):\n",
    "  def __init__(self, num_classes=2, train_base_model=False):\n",
    "      super().__init__()\n",
    "      weights = ResNet50_Weights.IMAGENET1K_V1\n",
    "      base = resnet50(weights=weights)\n",
    "\n",
    "      # optionally train or freeze base model parameters\n",
    "      for param in base.parameters():\n",
    "            param.requires_grad = train_base_model\n",
    "\n",
    "      self.base_model = nn.Sequential(*list(base.children()))[:-2] # base.children are all the modules within resent. remove avg pool and fc\n",
    "\n",
    "      self.new_model = nn.Sequential(\n",
    "       nn.Conv2d(2048, 512, kernel_size=5, padding=1),\n",
    "        nn.BatchNorm2d(512),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(2),          # â† downsample early (critical)\n",
    "\n",
    "        nn.Conv2d(512, 256, kernel_size=3, padding=1),\n",
    "        nn.BatchNorm2d(256),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.2),\n",
    "        nn.MaxPool2d(2),\n",
    "\n",
    "        nn.Conv2d(256, 128, kernel_size=3, padding=1),\n",
    "        nn.BatchNorm2d(128),\n",
    "        nn.LeakyReLU(0.01), # inplace=F\n",
    "        nn.Dropout(0.2),\n",
    "        nn.AdaptiveAvgPool2d((1,1)),\n",
    "\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(128, 64),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.4),\n",
    "        nn.Linear(64, num_classes)\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.base_model(x)\n",
    "    x = self.new_model(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WbYYWApUezii"
   },
   "source": [
    "##Train and test NEW model on original EmoSet Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "id": "lq1ZYD34A2p4",
    "outputId": "71c7ed6f-94d4-4e36-b0a5-ea3a09c51a65"
   },
   "outputs": [],
   "source": [
    "val_model = NapOasEmoResNet(num_classes=2, train_base_model=True)\n",
    "\n",
    "# use adam optimizer\n",
    "#Increase the learning rate\n",
    "optimizer = optim.Adam(val_model.parameters(), lr=1e-3)\n",
    "\n",
    "#Use Data Augmentation techniques to increase diversity!\n",
    "#train_dataset.set_transform(transform_batch_train_aug)\n",
    "train_dataset.set_transform(transform_wrapper(transforms=train_transform))\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "#Increase from 3 to 5 epochs\n",
    "train_EmoSet(train_loader, val_loader, val_model, optimizer, epochs=5)\n",
    "\n",
    "save_path = FOLDERNAME + \"finetuned_model_NapOas.pth\"\n",
    "\n",
    "# Recommended: save state_dict (weights only)\n",
    "torch.save(val_model.state_dict(), save_path)\n",
    "\n",
    "print(f\"Model saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9eW0BvB-8rWU",
    "outputId": "c6d0b2a9-f645-4e1f-96af-a342a4ca411d"
   },
   "outputs": [],
   "source": [
    "# Recreate the same model architecture\n",
    "val_model = NapOasEmoResNet(num_classes=2, train_base_model=True)\n",
    "save_path = FOLDERNAME + \"finetuned_model_NapOas.pth\"\n",
    "\n",
    "# Load weights\n",
    "val_model.load_state_dict(torch.load(save_path, map_location=device))\n",
    "val_model.to(device)\n",
    "val_model.eval()  # important: sets dropout/batchnorm to inference mode\n",
    "\n",
    "print(\"Model loaded and ready for inference.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RA9cgFcSA_MY",
    "outputId": "1e2c4606-39b2-43c7-b996-e2d4243e0020"
   },
   "outputs": [],
   "source": [
    "print(\"Checking accuracy on test set:\")\n",
    "check_accuracy_EmoSet(test_loader, val_model) # EmoSet test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TN0SwNUZ9pQx"
   },
   "source": [
    "## NOW lets try the NEW trained model on NPAS/OASIS VALIDATION model... again :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YzIa3F7c9rpz",
    "outputId": "7df1d66b-91bc-4d96-a3f8-0b9c43d0b6b3"
   },
   "outputs": [],
   "source": [
    "print(\"Checking accuracy on NAPS/OASIS set:\")\n",
    "check_accuracy_NAPSOASISVAL(val_loader_NAPS, val_model) # NAPS + OASIS emoset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kyrFKMH9e36C"
   },
   "source": [
    "#STEP 3: Pull in Arousal data from NAPS/OASIS, enhancing our best model prior to ensure good performance during classification!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6UgJ5YFHfOGs"
   },
   "source": [
    "##PREP data, doing train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7FWPIzRgZ0Dm",
    "outputId": "785a2bb1-7ced-4f03-a8eb-c9b73e6ef048"
   },
   "outputs": [],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wETiw_G9Z9MQ"
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iJQqYdLn6MqO"
   },
   "outputs": [],
   "source": [
    "# Adapt for predicting arousal and valence\n",
    "# train test split NAPS/OASIS\n",
    "\n",
    "# split\n",
    "Xtr, Xts = train_test_split(image_data_final, test_size=0.1, random_state=42)\n",
    "Xtr, Xval = train_test_split(Xtr, test_size=0.1, random_state=42)\n",
    "Xtr.reset_index(drop=True, inplace=True)\n",
    "Xts.reset_index(drop=True, inplace=True)\n",
    "Xval.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# create dataset class\n",
    "dataset_train = MyDataset(img_list=Xtr[\"image_path\"],image_name = Xtr[\"ID\"], valence = Xtr['All_ValenceM'], arousal = Xtr['All_ArousalM'], source = Xtr['source'], transform=train_transform, val_continuous = False)\n",
    "dataset_test = MyDataset(img_list=Xts[\"image_path\"],image_name = Xts[\"ID\"], valence = Xts['All_ValenceM'], arousal = Xts['All_ArousalM'], source = Xts['source'], transform=train_transform, val_continuous = False)\n",
    "dataset_val = MyDataset(img_list=Xval[\"image_path\"],image_name = Xval[\"ID\"], valence = Xval['All_ValenceM'], arousal = Xval['All_ArousalM'], source = Xval['source'], transform=train_transform, val_continuous = False)\n",
    "\n",
    "# create dataloader object\n",
    "train_loader_NAPS = DataLoader(dataset_train, batch_size=128, shuffle=False, collate_fn = collate_fn_NAPSOASIS)\n",
    "test_loader_NAPS = DataLoader(dataset_test, batch_size=128, shuffle=False, collate_fn = collate_fn_NAPSOASIS)\n",
    "val_loader_NAPS = DataLoader(dataset_val, batch_size=128, shuffle=False, collate_fn = collate_fn_NAPSOASIS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "09pBVvngfwJj"
   },
   "source": [
    "##Set up the NAPS/OASIS checks, ensuring that we account for 2 heads since we need to check for both valence AND accuracy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3ITiizaBFLuc"
   },
   "outputs": [],
   "source": [
    "value_range_V = image_data_final['All_ValenceM'].max() - image_data_final['All_ValenceM'].min()\n",
    "value_range_A = image_data_final['All_ArousalM'].max() - image_data_final['All_ArousalM'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UFyntqLu7VBU"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "def check_accuracy_NAPSOASIS(loader, model, N=256):\n",
    "\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    valence_acc = 0\n",
    "    num_samples = 0\n",
    "    arousal_acc = 0\n",
    "    arousal_mae_L1 = 0\n",
    "\n",
    "    all_aro_true = []\n",
    "    all_aro_pred = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            x = batch['pixel_values'].to(device)\n",
    "            y_valence = batch[\"valence\"].to(device=device, dtype=torch.long)\n",
    "            y_arousal = batch[\"arousal\"].to(device=device, dtype=torch.float)\n",
    "\n",
    "            valence_pred, arousal_pred = model(x)\n",
    "\n",
    "            # valence accuracy\n",
    "            _, preds = valence_pred.max(1)\n",
    "            valence_acc += (preds == y_valence).sum()\n",
    "            num_samples += preds.size(0)\n",
    "\n",
    "            # arousal accuracy\n",
    "            arousal_mae_L1 += F.l1_loss(arousal_pred, y_arousal, reduction='sum').item()\n",
    "\n",
    "            all_aro_true.extend(y_arousal.cpu().numpy().flatten())\n",
    "            all_aro_pred.extend(arousal_pred.cpu().numpy().flatten())\n",
    "\n",
    "    mae_aro = arousal_mae_L1 / num_samples\n",
    "\n",
    "    mae_aro_norm = mae_aro / value_range_A\n",
    "    mae_aro_acc_like = 100*(1 - mae_aro_norm)\n",
    "\n",
    "    aro_r2 = r2_score(all_aro_true, all_aro_pred)\n",
    "\n",
    "    val_acc = 100.0 * valence_acc / num_samples\n",
    "\n",
    "    print(f'Valence Accuracy: {val_acc:.4f}%\\n'\n",
    "          f'Arousal MAE Accuracy: {mae_aro_acc_like:.4f}%\\n'\n",
    "          f'Arousal Accuracy with R2: {aro_r2:.4f}'\n",
    "          )\n",
    "\n",
    "\n",
    "print_every = 100\n",
    "\n",
    "def train_NAPSOASIS(model, optimizer, epochs=1): #scale_param=0.6\n",
    "    model = model.to(device=device)  # move the model parameters to CPU/GPU\n",
    "    for e in range(epochs):\n",
    "        for t, batch in enumerate(train_loader_NAPS):\n",
    "            x = batch['pixel_values'].to(device)\n",
    "            y_valence = batch[\"valence\"].to(device=device, dtype=torch.long)\n",
    "            y_arousal = batch[\"arousal\"].to(device=device, dtype=torch.float)\n",
    "\n",
    "            valence_pred, arousal_pred = model(x)\n",
    "\n",
    "            #compute losses\n",
    "            loss_valence = F.cross_entropy(valence_pred, y_valence)\n",
    "            loss_arousal = F.mse_loss(arousal_pred, y_arousal)\n",
    "\n",
    "            # combine losswes\n",
    "            loss = loss_valence + loss_arousal # removed scale_param\n",
    "\n",
    "            # Zero out all of the gradients for the variables which the optimizer\n",
    "            # will update.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # This is the backwards pass: compute the gradient of the loss with\n",
    "            # respect to each  parameter of the model.\n",
    "            loss.backward()\n",
    "\n",
    "            # Actually update the parameters of the model using the gradients\n",
    "            # computed by the backwards pass.\n",
    "            optimizer.step()\n",
    "\n",
    "            if t % print_every == 0:\n",
    "                print(f\"Iter {t} | Total Loss: {loss.item():.4f} | Valence Loss: {loss_valence.item():.4f} | Arousal Loss: {loss_arousal.item():.4f}\")\n",
    "                check_accuracy_NAPSOASIS(val_loader_NAPS, model)\n",
    "                print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MeNC_k7p7ct_"
   },
   "outputs": [],
   "source": [
    "class ValandArousal(nn.Module):\n",
    "  def __init__(self, weight_path, num_classes=2, train_base_model=False): # consider True\n",
    "        super().__init__()\n",
    "\n",
    "        # 1. base model architecture\n",
    "        self.base = NapOasEmoResNet(num_classes=2, train_base_model=train_base_model)\n",
    "\n",
    "        # 2. pretrained weights\n",
    "        state = torch.load(weight_path, map_location=device)\n",
    "        self.base.load_state_dict(state)\n",
    "\n",
    "        # 3. optional train of base model\n",
    "        for p in self.base.parameters():\n",
    "            p.requires_grad = train_base_model\n",
    "\n",
    "        # backbone model\n",
    "        # get the layers of new_model EXCEPT the last four layers\n",
    "        new_layers = list(self.base.new_model.children())[:-5]\n",
    "\n",
    "        self.emoset_model = nn.Sequential(\n",
    "            self.base.base_model,   # full ResNet backbone\n",
    "            *new_layers             # your conv head up to Flatten()\n",
    "        )\n",
    "\n",
    "          #self.feature_dim = 256 #128 # After flatten: 128-dimensional feature vector\n",
    "\n",
    "        # valence head: this can just be the same as our model\n",
    "        self.valence_model = nn.Sequential(\n",
    "          nn.Flatten(),\n",
    "          nn.Linear(128, 64),\n",
    "          nn.ReLU(),\n",
    "          nn.Dropout(0.4),\n",
    "          nn.Linear(64, num_classes)\n",
    "        )\n",
    "\n",
    "        # arousal head:\n",
    "        self.arousal_model = Arousal_Model(in_channels=128) # consider adjusting and/or binarizing\n",
    "\n",
    "  def forward(self, x):\n",
    "        # shared backbone\n",
    "        x = self.emoset_model(x)   # â†’ (N,128) (64, 128)\n",
    "\n",
    "        # two heads\n",
    "        valence_out = self.valence_model(x)\n",
    "        arousal_out = self.arousal_model(x).squeeze(1)\n",
    "        return valence_out, arousal_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6xcl1J1dDHIX"
   },
   "outputs": [],
   "source": [
    "class Arousal_Model(nn.Module):\n",
    "    def __init__(self, in_channels=2048):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 512, kernel_size=5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(512, 1),   # in_features=512, out_features=1\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-ac1HDtPaknW",
    "outputId": "56c9bf90-95ac-46d7-b0c1-9e847718693f"
   },
   "outputs": [],
   "source": [
    "# define path where weights are stored\n",
    "weights_path = FOLDERNAME + \"finetuned_model_NapOas.pth\"\n",
    "\n",
    "# define model and optimizer\n",
    "ValandArousal_model_cat = ValandArousal(weight_path=weights_path)\n",
    "optimizer = optim.Adam(ValandArousal_model_cat.parameters(), lr=1e-3)\n",
    "\n",
    "# train and print val accuracy\n",
    "train_NAPSOASIS(ValandArousal_model_cat, optimizer, epochs=5)\n",
    "\n",
    "# save\n",
    "save_path = FOLDERNAME + \"finetuned_model_ValandArousal.pth\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SGOa_KW5BxNP",
    "outputId": "c5a5e844-2190-464b-a857-8bf9e3b33a89"
   },
   "outputs": [],
   "source": [
    "# # Recommended: save state_dict (weights only)\n",
    "torch.save(model.state_dict(), save_path)\n",
    "print(f\"Model saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z1ykx6ycj6BU",
    "outputId": "a9ad498c-c3c2-491c-80f8-a02b7f8bfafe"
   },
   "outputs": [],
   "source": [
    "print(\"Checking accuracy on test set:\")\n",
    "check_accuracy_NAPSOASIS(test_loader_NAPS, ValandArousal_model_cat) # Arousal Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q-eX9Hx2KLIL"
   },
   "outputs": [],
   "source": [
    "# create dataset class\n",
    "image_data_final, final_v_mean, final_v_stdv, final_a_mean, final_a_stdv = normalize_valence()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZB8V5IC_9pMV"
   },
   "outputs": [],
   "source": [
    "# split\n",
    "Xtr, Xts = train_test_split(image_data_final, test_size=0.2, random_state=42)\n",
    "Xtr, Xval = train_test_split(Xtr, test_size=0.4, random_state=42)\n",
    "Xtr.reset_index(drop=True, inplace=True)\n",
    "Xts.reset_index(drop=True, inplace=True)\n",
    "Xval.reset_index(drop=True, inplace=True)\n",
    "\n",
    "dataset_train = MyDataset(img_list=Xtr[\"image_path\"],image_name = Xtr[\"ID\"], valence = Xtr['All_ValenceM'], arousal = Xtr['All_ArousalM'], source = Xtr['source'], transform=train_transform, val_continuous = True)\n",
    "dataset_test = MyDataset(img_list=Xts[\"image_path\"],image_name = Xts[\"ID\"], valence = Xts['All_ValenceM'], arousal = Xts['All_ArousalM'], source = Xts['source'], transform=train_transform, val_continuous = True)\n",
    "dataset_val = MyDataset(img_list=Xval[\"image_path\"],image_name = Xval[\"ID\"], valence = Xval['All_ValenceM'], arousal = Xval['All_ArousalM'], source = Xval['source'], transform=train_transform, val_continuous = True)\n",
    "\n",
    "# create dataloader object\n",
    "train_loader_NAPS = DataLoader(dataset_train, batch_size=32, shuffle=False, collate_fn = collate_fn_NAPSOASIS)\n",
    "test_loader_NAPS = DataLoader(dataset_test, batch_size=32, shuffle=False, collate_fn = collate_fn_NAPSOASIS)\n",
    "val_loader_NAPS = DataLoader(dataset_val, batch_size=32, shuffle=False, collate_fn = collate_fn_NAPSOASIS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vaimNeA5KHoO"
   },
   "outputs": [],
   "source": [
    "value_range_V = image_data_final['All_ValenceM'].max() - image_data_final['All_ValenceM'].min()\n",
    "value_range_A = image_data_final['All_ArousalM'].max() - image_data_final['All_ArousalM'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZqVzMsp40aoo"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "def check_accuracy_NAPSOASISCON(loader, model, N=256):\n",
    "\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    num_samples = 0\n",
    "    arousal_mae_L1, valence_mae_L1 = 0, 0\n",
    "\n",
    "    all_val_true = []\n",
    "    all_val_pred = []\n",
    "    all_aro_true = []\n",
    "    all_aro_pred = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            x = batch['pixel_values'].to(device)\n",
    "            y_valence = batch[\"valence\"].to(device=device, dtype=torch.float)\n",
    "            y_arousal = batch[\"arousal\"].to(device=device, dtype=torch.float)\n",
    "\n",
    "            valence_pred, arousal_pred = model(x)\n",
    "\n",
    "            num_samples += y_valence.size(0) # batch size\n",
    "            valence_mae_L1 += F.l1_loss(valence_pred, y_valence, reduction='sum').item()\n",
    "            arousal_mae_L1 += F.l1_loss(arousal_pred, y_arousal, reduction='sum').item()\n",
    "\n",
    "            all_val_true.extend(y_valence.cpu().numpy().flatten())\n",
    "            all_val_pred.extend(valence_pred.cpu().numpy().flatten())\n",
    "            all_aro_true.extend(y_arousal.cpu().numpy().flatten())\n",
    "            all_aro_pred.extend(arousal_pred.cpu().numpy().flatten())\n",
    "\n",
    "    mae_val = valence_mae_L1 / num_samples\n",
    "    mae_aro = arousal_mae_L1 / num_samples\n",
    "    mae_val_norm = mae_val / value_range_V\n",
    "    mae_aro_norm = mae_aro / value_range_A\n",
    "    mae_val_acc_like = 100*(1 - mae_val_norm)\n",
    "    mae_aro_acc_like = 100*(1 - mae_aro_norm)\n",
    "\n",
    "    val_r2 = r2_score(all_val_true, all_val_pred)\n",
    "    aro_r2 = r2_score(all_aro_true, all_aro_pred)\n",
    "\n",
    "    print(f'Valence Accuracy MAE: {mae_val_acc_like:.4f}%\\n'\n",
    "          f'Arousal Accuracy MAE: {mae_aro_acc_like:.4f}%\\n'\n",
    "          f'Valence Accuracy R2: {val_r2:.4f}\\n'\n",
    "          f'Arousal Accuracy R2: {aro_r2:.4f}\\n'\n",
    "          )\n",
    "\n",
    "\n",
    "print_every = 100\n",
    "\n",
    "def train_NAPSOASISCON(model, optimizer, epochs=1): #scale_param=0.6\n",
    "    model = model.to(device=device)  # move the model parameters to CPU/GPU\n",
    "    for e in range(epochs):\n",
    "        for t, batch in enumerate(train_loader_NAPS):\n",
    "            x = batch['pixel_values'].to(device)\n",
    "            y_valence = batch[\"valence\"].to(device=device, dtype=torch.float)\n",
    "            y_arousal = batch[\"arousal\"].to(device=device, dtype=torch.float)\n",
    "\n",
    "            valence_pred, arousal_pred = model(x)\n",
    "\n",
    "            #compute losses\n",
    "            loss_valence = F.mse_loss(valence_pred, y_valence)\n",
    "            loss_arousal = F.mse_loss(arousal_pred, y_arousal)\n",
    "\n",
    "            # combine losswes\n",
    "            loss = loss_valence + loss_arousal # removed scale_param\n",
    "\n",
    "            # Zero out all of the gradients for the variables which the optimizer\n",
    "            # will update.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # This is the backwards pass: compute the gradient of the loss with\n",
    "            # respect to each  parameter of the model.\n",
    "            loss.backward()\n",
    "\n",
    "            # Actually update the parameters of the model using the gradients\n",
    "            # computed by the backwards pass.\n",
    "            optimizer.step()\n",
    "\n",
    "            if t % print_every == 0:\n",
    "                print(f\"Iter {t} | Total Loss: {loss.item():.4f} | Valence Loss: {loss_valence.item():.4f} | Arousal Loss: {loss_arousal.item():.4f}\")\n",
    "                check_accuracy_NAPSOASISCON(val_loader_NAPS, model)\n",
    "                print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vlYG0Ir80oSx"
   },
   "outputs": [],
   "source": [
    "# same as Arousal_Model just renamed for clarity... will clean up after testing\n",
    "\n",
    "class Continuous_Output_Model(nn.Module):\n",
    "    def __init__(self, in_channels=2048):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 512, kernel_size=5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(512, 1),   # in_features=512, out_features=1\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P-PDWHd-0udr"
   },
   "outputs": [],
   "source": [
    "class ValandArousalCON(nn.Module):\n",
    "  def __init__(self, weight_path, num_classes=2, train_base_model=False): # consider True\n",
    "        super().__init__()\n",
    "\n",
    "        # 1. base model architecture\n",
    "        self.base = NapOasEmoResNet(num_classes=num_classes, train_base_model=train_base_model)\n",
    "\n",
    "        # 2. pretrained weights\n",
    "        state = torch.load(weight_path, map_location=device)\n",
    "        self.base.load_state_dict(state)\n",
    "\n",
    "        # 3. optional train of base model\n",
    "        for p in self.base.parameters():\n",
    "            p.requires_grad = train_base_model\n",
    "\n",
    "        # backbone model\n",
    "        # get the layers of new_model EXCEPT the last four layers\n",
    "        new_layers = list(self.base.new_model.children())[:-5]\n",
    "\n",
    "        self.emoset_model = nn.Sequential(\n",
    "            self.base.base_model,   # full ResNet backbone\n",
    "            *new_layers             # conv head up to Flatten()\n",
    "        )\n",
    "\n",
    "          #self.feature_dim = 256 #128 # After flatten: 128-dimensional feature vector\n",
    "\n",
    "        # valence head: this can just be the same as our model\n",
    "        self.valence_model = Continuous_Output_Model(in_channels=128)\n",
    "\n",
    "        # arousal head:\n",
    "        self.arousal_model = Continuous_Output_Model(in_channels=128) # consider adjusting and/or binarizing\n",
    "\n",
    "  def forward(self, x):\n",
    "        # shared backbone\n",
    "        x = self.emoset_model(x)   # â†’ (N,128) (64, 128)\n",
    "\n",
    "        # two heads\n",
    "        valence_out = self.valence_model(x).squeeze(1)\n",
    "        arousal_out = self.arousal_model(x).squeeze(1)\n",
    "        return valence_out, arousal_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OpVGWcAh2Lrt",
    "outputId": "b498788d-0157-48e5-e1ef-bf96c0be9da0"
   },
   "outputs": [],
   "source": [
    "# define path where weights are stored\n",
    "weights_path = FOLDERNAME + \"finetuned_model_NapOas.pth\"\n",
    "\n",
    "# define model and optimizer\n",
    "ValandArousal_model = ValandArousalCON(weight_path=weights_path)\n",
    "optimizer = optim.Adam(ValandArousal_model.parameters(), lr=1e-3)\n",
    "\n",
    "# train and print val accuracy\n",
    "train_NAPSOASISCON(ValandArousal_model, optimizer, epochs=5)\n",
    "\n",
    "# save\n",
    "save_path = FOLDERNAME + \"finetuned_model_ValandArousalCON.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cznKcAet2c1J",
    "outputId": "b387f60a-7237-43ed-bcc7-4188f98df58b"
   },
   "outputs": [],
   "source": [
    "print(\"Checking accuracy on test set:\")\n",
    "check_accuracy_NAPSOASISCON(test_loader_NAPS, ValandArousal_model) # Arousal Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wYJDkk1Nq-4S"
   },
   "source": [
    "#STEP 4: Exploratory Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ofLkMAzErEyq"
   },
   "source": [
    "With batchsize = 32 + 1 epochs + lr - 1e5 + test size = 0.2, val size = 0.4 + No feature Selection + both are Regression based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iuSvWSZOMGTM",
    "outputId": "65d4f3d2-eb0b-423a-9cfe-db47759922f1"
   },
   "outputs": [],
   "source": [
    "save_location_folder = FOLDERNAME + \"YT_images_exploratory/\"\n",
    "\n",
    "# combine all exploratory images into one list of urls\n",
    "radesky_images_folder = FOLDERNAME + \"Radesky_YT_Dataset\"\n",
    "all_images = [save_location_folder, radesky_images_folder]\n",
    "\n",
    "final_images = []\n",
    "for folder in all_images:\n",
    "  final_images.extend(load_file(folder, [\".jpg\",\".png\"]))\n",
    "\n",
    "print(f\"Number of images: {len(final_images)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "PSJWppFhPx4D",
    "outputId": "16d85cfb-da2d-4951-f360-8acb30869736"
   },
   "outputs": [],
   "source": [
    "# pull in final annotations\n",
    "yt_annotated = FOLDERNAME + \"final_annotations.csv\"\n",
    "yt_annot = pd.read_csv(yt_annotated)\n",
    "yt_annot.head()\n",
    "\n",
    "# convert list into df\n",
    "final_images_df = pd.DataFrame(final_images, columns=[\"url\"])\n",
    "\n",
    "# create column with the id\n",
    "for url in final_images_df[\"url\"]:\n",
    "  id = url.split(\"/\")[-1].split(\".\")[0]\n",
    "  # make id a new column\n",
    "  final_images_df.loc[final_images_df[\"url\"] == url, \"id\"] = id\n",
    "\n",
    "final_images_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "8K8sUy5DL3_a",
    "outputId": "9b975afe-37d6-4e76-be66-8aecf6bade1b"
   },
   "outputs": [],
   "source": [
    "# merge yt_annot and final_images_df\n",
    "exploratory_set = pd.merge(yt_annot, final_images_df, on=\"id\", how=\"inner\")\n",
    "exploratory_set.drop(columns=[\"true_valence\", \"true_arousal\"], inplace=True)\n",
    "exploratory_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "NqvMFTqIL5pb",
    "outputId": "ebdb5271-2e34-4223-93db-3b18424ddf3b"
   },
   "outputs": [],
   "source": [
    "#NORMALIZE valence and arousal using NAPS (since ranges are the same)\n",
    "exploratory_set[\"valence\"] = (exploratory_set[\"valence\"] - final_v_mean)/final_v_stdv\n",
    "exploratory_set[\"arousal\"] = (exploratory_set[\"arousal\"] - final_a_mean)/final_a_stdv\n",
    "exploratory_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qz-thCieVgW6"
   },
   "outputs": [],
   "source": [
    "#FIND THE mid point to set our references!\n",
    "thresh_val = (abs(exploratory_set[\"valence\"].max())-abs(exploratory_set[\"valence\"].min()))/2\n",
    "thresh_arou = (abs(exploratory_set[\"arousal\"].max())-abs(exploratory_set[\"arousal\"].min()))/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1cF0TXQ9TzNd"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "def check_accuracy_Exploratory(loader, model, N=256, type_val = \"con\"):\n",
    "\n",
    "    incorrect_url = []\n",
    "\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    num_samples = 0\n",
    "    censor_acc = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            x = batch['pixel_values'].to(device)\n",
    "            y_valence=batch[\"valence\"].to(device=device, dtype=torch.float)\n",
    "            if type_val == \"con\":\n",
    "              y_valence = (batch[\"valence\"]<thresh_val).to(device=device, dtype=torch.float)\n",
    "            y_arousal = (batch[\"arousal\"]<thresh_arou).to(device=device, dtype=torch.float)\n",
    "\n",
    "            acc_censor = ((y_valence == 1) & (y_arousal == 0)).float()\n",
    "\n",
    "            valence_pred, arousal_pred = model(x)\n",
    "\n",
    "            preds_valence = None\n",
    "            if type_val == \"con\":\n",
    "              preds_valence = (torch.sigmoid(valence_pred) < thresh_val).long()\n",
    "            else:\n",
    "              preds_valence = torch.argmax(valence_pred, dim=1)\n",
    "            preds_arousal = (torch.sigmoid(arousal_pred) < thresh_arou).long()\n",
    "\n",
    "            pred_censor = ((preds_valence == 1) & (preds_arousal == 0)).float()\n",
    "\n",
    "            num_samples += y_valence.size(0) # batch size\n",
    "            censor_acc += (pred_censor == acc_censor).sum()\n",
    "\n",
    "            urls = batch[\"image_path\"]\n",
    "            incorrect_mask = pred_censor != acc_censor\n",
    "            incorrect_indices = incorrect_mask.nonzero(as_tuple=True)[0].tolist()\n",
    "            incorrect_url.extend([urls[x] for x in incorrect_indices])\n",
    "\n",
    "            ret_clown = None\n",
    "            ret_car = None\n",
    "            ret_clown = [incorrect_mask[i] for i,x in enumerate(urls) if \"7pe0Q8LqiO4\" in x]\n",
    "            ret_car = [incorrect_mask[i] for i,x in enumerate(urls) if \"e7VUBJdyw2k\" in x]\n",
    "\n",
    "            print(f\"Clown Censored: {ret_clown}\")\n",
    "            print(f\"Car Censored: {ret_car}\")\n",
    "            for i,x in enumerate(urls):\n",
    "              if \"im_\" in x:\n",
    "                print(f\"Image {x} Censored: {incorrect_mask[i]}\")\n",
    "\n",
    "\n",
    "    censor_accuray = (censor_acc/num_samples)*100\n",
    "\n",
    "    print(f'Censor Accuracy : {censor_accuray:.4f}%')\n",
    "\n",
    "    return incorrect_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_s6F7vbzl2cK"
   },
   "outputs": [],
   "source": [
    "# Define custom collate_fn to ignore non-tensor fields / tconvert arrays to tensors\n",
    "def collate_fn_Explore(batch): # batch is a list of dictionaries\n",
    "    pixel_values = torch.stack([item[\"pixel_values\"] for item in batch])\n",
    "    valence = torch.tensor([item[\"valence\"] for item in batch])\n",
    "    arousal = torch.tensor([item[\"arousal\"] for item in batch])\n",
    "    image_path = [item[\"image_path\"] for item in batch]\n",
    "\n",
    "    return {\"pixel_values\": pixel_values, \"valence\": valence, \"arousal\": arousal, \"image_path\": image_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rz9Lufh-L8pJ",
    "outputId": "5ce833f6-368d-4b0a-cb14-491166a904e0"
   },
   "outputs": [],
   "source": [
    "dataset_yt = MyDataset(img_list=exploratory_set[\"url\"],image_name = exploratory_set[\"id\"], valence = exploratory_set[\"valence\"], arousal = exploratory_set[\"arousal\"], source = None, transform=val_transform, val_continuous = True)\n",
    "val_loader_yt = DataLoader(dataset_yt, batch_size=32, shuffle=False, collate_fn = collate_fn_Explore)\n",
    "\n",
    "# check accuracy\n",
    "check_accuracy_NAPSOASISCON(val_loader_yt, ValandArousal_model, N=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wYE0bnNzR1VR"
   },
   "source": [
    "###CHECK BELOW FOR YOUR RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bRuYeFfbZNQp",
    "outputId": "34f204dd-50a9-4fd3-8830-9b35de03f2d6"
   },
   "outputs": [],
   "source": [
    "incorrect_images_Con = check_accuracy_Exploratory(val_loader_yt, ValandArousal_model, N=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dTvfo_lGzoNb",
    "outputId": "85218ed2-8b7b-4d9c-825c-b55fade06168"
   },
   "outputs": [],
   "source": [
    "print(incorrect_images_Con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2ZqCfgGYaJQG"
   },
   "outputs": [],
   "source": [
    "exploratory_set[\"source\"] = \"Explore\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dquXGoqxNX2m",
    "outputId": "a6af5a61-7ca2-4a52-fff3-ac7140e10840"
   },
   "outputs": [],
   "source": [
    "dataset_yt = MyDataset(img_list=exploratory_set[\"url\"],image_name = exploratory_set[\"id\"], valence = exploratory_set[\"valence\"], arousal = exploratory_set[\"arousal\"], source = exploratory_set[\"source\"], transform=val_transform, val_continuous = False)\n",
    "val_loader_yt = DataLoader(dataset_yt, batch_size=32, shuffle=False, collate_fn = collate_fn_Explore)\n",
    "\n",
    "# check accuracy\n",
    "check_accuracy_NAPSOASIS(val_loader_yt, ValandArousal_model_cat, N=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zMv5dXEkgzaK"
   },
   "source": [
    "Recall the extremely negative = 1 and extremely positive = 0 for valence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j278yluiZ8HZ",
    "outputId": "d5ed1b4c-3f9a-470a-a136-a109509cdb27"
   },
   "outputs": [],
   "source": [
    "incorrect_images_Cat = check_accuracy_Exploratory(val_loader_yt, ValandArousal_model_cat, N=32, type_val = \"cat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KcPP-fDKz9Hd",
    "outputId": "5f7e5f60-48fd-43ec-f02d-d269bbf7ddb9"
   },
   "outputs": [],
   "source": [
    "print(incorrect_images_Cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ocZ66rBo0gi0"
   },
   "outputs": [],
   "source": [
    "overlap_img = [x for x in incorrect_images_Cat if x in incorrect_images_Con]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gw8vTcVm0563"
   },
   "outputs": [],
   "source": [
    "def load_image(entry):\n",
    "  with open(entry, \"rb\") as f:\n",
    "            return f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "pCAjRah-0qTM",
    "outputId": "43b9d7e0-0128-4ec1-9cc1-bf70b370fdf6"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image as img2, display\n",
    "\n",
    "for entry in overlap_img:\n",
    "  img_bytes = load_image(entry)  # returns f.read()\n",
    "  display(img2(data=img_bytes))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
